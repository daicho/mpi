\documentclass[a4j,titlepage]{jsarticle}

\usepackage[dvipdfmx]{graphicx,xcolor}
\usepackage[top=20truemm,left=25truemm,right=25truemm]{geometry}
\usepackage{amsmath}
\usepackage{here}
\usepackage{comment}
\usepackage{url}
\usepackage{plistings}
\usepackage{tikz}
\usepackage[framemethod=tikz]{mdframed}

\renewcommand{\lstlistingname}{リスト}

\newcommand{\chuo}[1]{\multicolumn{1}{|c|}{#1}}
\newcommand{\inpt}[1]{\underline{#1}\,\setlength{\fboxsep}{1pt}\fbox{\small ↓}}
\newcommand{\bvec}[1]{\mbox{\boldmath $#1$}}

\lstdefinestyle{C}{
  language=C,
  basicstyle=\small\ttfamily,
  keywordstyle=\color[HTML]{0000E0},
  stringstyle=\color[HTML]{A31515},
  commentstyle=\upshape\color[HTML]{008000},
  frame=trbl,
  framesep=5pt,
  columns=[l]{fullflexible},
  numbers=left,
  xleftmargin=3zw,
  lineskip=-0.2ex,
  breaklines=true,
  showstringspaces=false,
  tabsize=4,
  keepspaces=true
}

\lstdefinestyle{make}{
  language=,
  basicstyle=\small\ttfamily,
  keywordstyle=\color[HTML]{0000E0},
  stringstyle=\color[HTML]{A31515},
  commentstyle=\upshape\color[HTML]{008000},
  frame=trbl,
  framesep=5pt,
  columns=[l]{fullflexible},
  numbers=left,
  xleftmargin=3zw,
  lineskip=-0.2ex,
  breaklines=true,
  showstringspaces=false,
  tabsize=4,
  keepspaces=true
}

\lstdefinestyle{text}{
  language=,
  basicstyle=\ttfamily,
  frame=trbl,
  framesep=5pt,
  columns=[l]{fullflexible},
  xleftmargin=3zw,
  lineskip=-0.2ex,
  showstringspaces=false,
  tabsize=4,
  keepspaces=true
}

\mdfsetup{
  skipabove=5pt,
  innertopmargin=10pt,
  innerbottommargin=10pt,
  roundcorner=10pt,
  font=\ttfamily
}


\begin{document}


\begin{titlepage}
  \title{\huge{工学実験実習V} \\ \LARGE{---MPI---}}
	\author{学籍番号：16426 \\ 5年 電子情報工学科 24番 \\ 福澤 大地}
	\date{提出日 : 2020年10月19日}
  \maketitle
\end{titlepage}


\section{目的}
MPI (Message Passing Interface)を用いて、複数のCPU間で通信を行いながら、
並列計算を行うプログラムを作成する。
その上で、並列計算を行うと通常に比べどれほどの高速化を図れるか、
並列計算に適したアルゴリズムとはどのようなものなのかなどを検証する。


\section{実験環境}
プログラムの開発、実行を行った環境を表\ref{tb:kan}に示す。
表\ref{tb:kan}と同様の環境のコンピュータ44台が同一ネットワーク内に接続されており、
公開鍵認証方式でこれらのコンピュータとSSH通信を行える環境で実験を行った。

\begin{table}[H]
  \centering
  \caption{実験環境}
  \label{tb:kan}

  \begin{tabular}{|l|l|}
    \hline
    CPU & Intel Core i5-6600 @ 3.3GHz \\ \hline
    メモリ & 8GB \\ \hline
    OS & Ubuntu 14.04 LTS \\ \hline
    システム & 64bit \\ \hline
    コンパイラ & GCC 4.8.4 \\ \hline
    MPIライブラリ & Open MPI 1.10.2 \\ \hline
  \end{tabular}
\end{table}


\section{MPIとOpen MPIについて}
MPIとは、並列計算を行うために標準化された規格である。
これを用いることにより、1個のCPUで行っていた計算を複数のCPUで分散して行えるようになる。

Open MPI \cite{bib:1} は、MPIに準拠したライブラリの1つであり、Unix上で利用できる。
MPIのライブラリは他にもMPICH \cite{bib:2} などがあるが、本実験ではOpen MPIを使用する。


\section{実行方法}
プログラムのコンパルには\texttt{mpicc}コマンド、実行には\texttt{mpirun}コマンドを使用する。
\texttt{mpicc}コマンドは\texttt{gcc}コマンドと同様の使い方ができ、
\texttt{-Wall}オプションなどを利用することもできる。
\texttt{mpirun}コマンドは、\texttt{-machinefile}オプションで使用するコンピュータの名前とCPUの数が記述されたファイル名を、
\texttt{-np}オプションで使用するCPUの数を指定することで、コンパイルしたファイルを実行することができる。

例えば、``com001'' $\sim$ ``com004''という名前のコンピュータのCPUを1つずつ使用する場合は、
次のように記述されたテキストファイルを適当なファイル名で保存する。
ここでは、ホームディレクトリに``mymachines''というファイル名で保存することとする。

\begin{lstlisting}[style=text]
com001 cpu=1
com002 cpu=1
com003 cpu=1
com004 cpu=1
\end{lstlisting}

そして、``program.c''というファイル名のプログラムをコンパイルし、
先ほど指定した$4$個のCPU実行する場合には次のようなコマンドを入力する。

\begin{lstlisting}[style=text]
$ mpicc program.c
$ mpirun -machinefile ~/mymachines -np 4 ./a.out
\end{lstlisting}

なお、今回の環境では次のようにエイリアスを設定することにより、\texttt{-machinefile}オプションを省略し実行できるようにしてある。

\begin{lstlisting}[style=text]
alias mpirun='-machinefile ~/mymachines'
\end{lstlisting}


\section{MPIのプログラム}
MPIを用いてプログラムを作成する際は、通常のプログラムとは違い、
今実行しているCPUの数はいくつなのか、自分はどのCPUなのかなどの情報を取得する必要がある。
そのため、MPIのプログラムではリスト\ref{lst:mpi}のように、
前処理を行うプログラムを記述する必要がある。
なお、プログラムの終了時には、必ず\texttt{MPI\_Finalize}関数を呼び出さなければならない。

\lstinputlisting[style=C,caption=MPIのプログラム,label=lst:mpi]{./MPI/test.c}

リスト\ref{lst:mpi}のプログラムを4個のCPUで実行した結果を、リスト\ref{lst:mpikekka}に示す。
リスト\ref{lst:mpikekka}を見ると、\texttt{nsize}に実行しているCPUの数、\texttt{myrank}に自身の番号、\texttt{my\_name}に自身のコンピュータ名が入っていることが分かる。
実行結果が\texttt{myrank}の順番で表示されていないのは、プログラムが各CPU上で同時に実行されているためである。

\lstinputlisting[style=text,caption=MPIのプログラムの実行結果,label=lst:mpikekka]{./MPI/result/test.txt}


\section{課題1}
\subsection{課題内容}
コマンドライン引数から数値$X$を受け取り、$1 \sim X$までの和を$N$台のCPUで求めるプログラムを作成する。
$X$と$N$は任意の自然数とする。

\subsection{プログラムリスト}
課題1のプログラムを、リスト\ref{lst:kadai1}に示す。

\lstinputlisting[style=C,caption=課題1のプログラム,label=lst:kadai1]{./MPI/kadai01.c}

\subsection{プログラムの説明}
\subsubsection{エラーチェック}
$26 \sim 47$行目では、与えられた引数が正しいものであるのかチェックを行っている。

本プログラムでは計算をlong long int型で行っている。
long long int型は64ビットであるため、表せる値の最大値は、$2^{63} - 1$である。
最終的な計算結果がこの範囲に収まっている必要があるので、
入力として許容できる最大値を$n$とすると、式(\ref{eq:range})のようにして求められる。

\begin{align}
  \begin{aligned}
    \sum^{n}_{k = 1} k &= 2^{63} - 1 \\
    \frac{1}{2} n (n + 1) &= 2^{63} - 1 \\
    \frac{1}{2} n^2 + \frac{1}{2} n - 2^{63} + 1 &= 0 \\
    n &\simeq \pm 4.3 \times 10^9
    \label{eq:range}
  \end{aligned}
\end{align}

式(\ref{eq:range})より、$n$が$4 \times 10^9$以内であれば確実にオーバーフローが起こることはないため、
これより大きい値が入力された際はエラーとしてプログラムを終了している。
また、コマンドライン引数が与えられていなかった場合や、
入力された値が0以下であった場合も同様にエラーとしてプログラムを終了している。

\subsubsection{演算}
演算は$51 \sim 53$行目で行っており、\texttt{myrank}$+1$から始め、\texttt{nsize}間隔で数字を足している。
例えば、4個のCPUで実行した場合には、各CPUが担当する数字は次のようになる。
このようにすることで、各CPUで担当する数字の個数と合計のばらつきを少なくしている。

\begin{description}
  \item[CPU 0] 1, 5, 9,  13, 17, $\dots$
  \item[CPU 1] 2, 6, 10, 14, 18, $\dots$
  \item[CPU 2] 3, 7, 11, 15, 19, $\dots$
  \item[CPU 3] 4, 8, 12, 16, 20, $\dots$
\end{description}

\subsubsection{集計}
各CPUで行った計算結果の集計は、55行目の\texttt{MPI\_Reduce}関数で行っている。
このような記述を行うことで、全てのCPUの\texttt{sum}の合計を、
\texttt{ans}に代入することができる。

第4パラメータには演算の種類を指定することができ、56行目の\texttt{MPI\_MAX}では最大値、
57行目の\texttt{MPI\_MIN}では最小値を取得することができる。

\subsubsection{処理時間の計測}
MPIには、過去のある地点からの経過時間を取得する\texttt{MPI\_Wtime}関数が用意されている。
この関数を処理の開始時と終了時に呼び出し、その差分を取ることで、
処理に掛かった時間を計測することができる。
本プログラムでは、49行目で開始時間、59行目で終了時間を取得し、65行目でその差分を表示している。

\subsection{実行結果}
\subsubsection{$1 \sim 54321$の和}
引数に54321を指定し、4個のCPUで実行した場合の結果をリスト\ref{lst:kadai1-1}に示す。

\subsubsection{大きな数を指定した場合}
入力できる最大値である、$4 \times 10^9$を引数に指定した場合の結果をリスト\ref{lst:kadai1-2}に示す。
なお、4個のCPUで実行した。

\subsubsection{エラーチェック}
引数を指定しなかった場合の結果をリスト\ref{lst:nohiki}、0以下の数を指定した場合の結果をリスト\ref{lst:underzero}、
$4 \times 10^9$を上回る数を指定した場合の結果をリスト\ref{lst:toobig}に示す。

\subsection{考察}


\section{課題2}
\subsection{課題内容}
課題1で作成したプログラムを用いて、並列計算の効果を測定する。
CPUの数$N$を変えながら処理時間を測定し、その結果をグラフにして考察する。

\subsection{実行結果}
CPUの数$N$を1$\sim$30個に増やしながら処理時間を測定した。
全てのCPUが作業が行われていない状態で測定を行い、それぞれ10回の平均値を取った。

それぞれのCPUの個数についての処理時間についてまとめたものを表\ref{tb:kadai2}に、グラフにプロットしたものを図\ref{fig:kadai2}に示す。

\begin{table}[H]
  \centering
  \caption{CPUの個数と処理時間}
  \label{tb:kadai2}

  \begin{tabular}{|r|r|r|}
    \hline
    \chuo{CPU数 [個]} & \chuo{処理時間 [ms]} & \chuo{CPU1個の処理時間に対する短縮率} \\ \hline \hline
     1 & 7663 &  1.00 \\ \hline
     2 & 3842 &  1.99 \\ \hline
     3 & 2572 &  2.98 \\ \hline
     4 & 1929 &  3.97 \\ \hline
     5 & 1544 &  4.96 \\ \hline
     6 & 1289 &  5.94 \\ \hline
     7 & 1113 &  6.89 \\ \hline
     8 &  971 &  7.89 \\ \hline
     9 &  862 &  8.89 \\ \hline
    10 &  780 &  9.83 \\ \hline
    11 &  711 & 10.78 \\ \hline
    12 &  652 & 11.76 \\ \hline
    13 &  613 & 12.49 \\ \hline
    14 &  576 & 13.29 \\ \hline
    15 &  536 & 14.29 \\ \hline
    16 &  492 & 15.58 \\ \hline
    17 &  466 & 16.46 \\ \hline
    18 &  440 & 17.42 \\ \hline
    19 &  415 & 18.48 \\ \hline
    20 &  397 & 19.29 \\ \hline
    21 &  391 & 19.61 \\ \hline
    22 &  370 & 20.71 \\ \hline
    23 &  346 & 22.17 \\ \hline
    24 &  328 & 23.36 \\ \hline
    25 &  319 & 24.05 \\ \hline
    26 &  309 & 24.83 \\ \hline
    27 &  300 & 25.51 \\ \hline
    28 &  296 & 25.91 \\ \hline
    29 &  282 & 27.19 \\ \hline
    30 &  266 & 28.77 \\ \hline
  \end{tabular}
\end{table}


\begin{figure}[H]
  \centering
  \includegraphics[width=12cm]{./MPI/kadai02-result/kadai02.png}
  \caption{CPUの個数と処理時間の関係}
  \label{fig:kadai2}
\end{figure}

\subsection{考察}
図\ref{fig:kadai2}を見ると、反比例のグラフとなっている。
また、表\ref{tb:kadai2}のCPU1個の処理時間に対する短縮率より、おおよそCPUの数の分だけ処理時間の短縮がされていることが分かる。
このことから、課題1のような、ほとんど通信を行わない単純な並列計算を行うと、投入したCPUの数の分だけ効率化が行えると言える。


\section{課題3}
\subsection{課題内容}
$N$個のCPUでモンテカルロシミュレーションを並列処理するプログラムを作成する。
乱数の種はCPUごとに異なるようにする。

\subsection{プログラムリスト}
課題3のプログラムを、リスト\ref{lst:kadai3}に示す。

\lstinputlisting[style=C,caption=課題3のプログラム,label=lst:kadai3]{./MPI/kadai03.c}

\subsection{プログラムの説明}
\subsubsection{エラーチェック}
$30 \sim 55$行目では、与えられた引数が正しいものであるのかチェックを行っている。

本プログラムでは、円を分割したものを各CPUで分担してシミュレーションを行うため、
各CPUでプロットする点は等しい必要がある。
そこで、コマンドライン引数から受け取った値がCPUの個数の倍数でない場合は、エラーとしてプログラムを終了している。
また、コマンドライン引数が与えられていなかった場合や、
入力された値が0以下であった場合も同様にエラーとしてプログラムを終了している。

\sebsubsection{シード値の決定}
ランダムな点をプロットするために、各CPUでシード値を決定する必要がある。
通常のプログラムであれば、現在時刻をシード値とするのが一般的であるが、並列計算のプログラムでその手法を取ると、各CPUでシード値が同じとなってしまう可能性がある。
同じシード値となると各CPUで全く同じ乱数列が生成されることとなるため、ランダム性が失われてしまうという問題がある。

そこで本プログラムでは、$61 \sim 66$行目のように各CPUについて、時刻をシード値とし\texttt{myrank}の回数だけ乱数を生成したものを新たなシード値として採用することでこの問題を解決している。
例えば、CPU0とCPU1が同じシード値$T$となってしまったとする。
この場合、CPU0では$T$をシード値として生成された1つ目の乱数を新しいシード値として採用し、
CPU1では2つ目の乱数を新しいシード値として採用する。
こうすることで、シード値として使う時刻が同じ値になってしまった場合にも、各CPUで違うシード値を使用することができる。

\subsubsection{シミュレーション}

\subsection{実行結果}

\subsection{考察}


\section{課題4}
\subsection{課題内容}
以下の処理を実行するプログラムを作成する。

\begin{enumerate}
  \item 整数配列\texttt{a\[\] = \{3, 1, 4, 1, 5, 9\}}をCPU 0で定義する。
  \item CPU$1 \sim 9$のそれぞれで適当な乱数\texttt{R}を1個ずつ発生させる。
  \item \texttt{a\[\]}を9台のCPUに\texttt{MPI\_Send}で送信し、
        受信側では\texttt{a\[\]}のそれぞれの要素に手順2で発生させた$R$を加えた配列\texttt{b\[\]}を作る。
  \item 9台のCPUからそれぞれが持っている\texttt{R}と\texttt{b}をCPU 0に送り返す。
  \item CPU 0でCPU番号とともに送り返されてきた\texttt{R}と\texttt{b}を表示する。
\end{enumerate}

\subsection{プログラムリスト}

\subsection{プログラムの説明}

\subsection{実行結果}

\subsection{考察}


\section{課題5}
\subsection{課題内容}

\subsection{プログラムリスト}

\subsection{プログラムの説明}

\subsection{実行結果}

\subsection{考察}


\section{課題6}
\subsection{課題内容}

\subsection{プログラムリスト}

\subsection{プログラムの説明}

\subsection{実行結果}

\subsection{考察}



\begin{thebibliography}{9}
  \bibitem{bib:1} OpenMPI: Open Source High Performance Computing, \texttt{\url{https://www.open-mpi.org/}}
  \bibitem{bib:2} MPICH $|$ High-Performance Portable MPI, \texttt{\url{https://www.mpich.org/}}
\end{thebibliography}


\end{document}
